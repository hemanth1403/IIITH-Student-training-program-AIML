{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hemanth1403/IIITH-Student-training-program-AIML/blob/main/AIML_Module_3_Lab_3_Using_KNN_for_Text_Classification_ipynb.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAKrrEmgLZ_J"
      },
      "source": [
        "### MODULE: CLASSIFICATION-1\n",
        "### LAB-3 : Using KNN for Text Classification\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jeRAsurrdLY"
      },
      "source": [
        "## **Section 1: Understanding NLP tools**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB9N2m-HrgWY"
      },
      "source": [
        "In this lab we will be using KNN on a real world NLP application i.e. is text classification. But first look at some NLP techniques for text classification and tools that we use when we want to use python for NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMJhTF89MmT_"
      },
      "source": [
        "## Section 1.2: Data Cleaning and Preprocessing step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRXVjJHk3urf"
      },
      "source": [
        "Raw text must be processed and converted into a form so that it is suitable to use with various machine-learning algorithms.  \n",
        "In case of text, there are lots of things that need to be taken into account.  \n",
        "\n",
        "\n",
        "1.   Removing numbers from the text\n",
        "2.   Handling capitalization and punctuation.\n",
        "3.   Stemming and Lemmatizing text.  \n",
        "\n",
        "And most importantly, one can't just use words or images directly in algorithms; they need to be converted into vectors- a form that algorithms can understand.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYq4np0xqUBr"
      },
      "source": [
        "### **NLTK**\n",
        "NLTK (or Natural Language Tool Kit) is a commonly used library for processing text. We will use this tool in this lab. Lets first install it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFOZHWV3_ABt",
        "outputId": "08f2a54a-3c08-42c3-e049-04f5c8e122f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/hemu/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/hemu/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/hemu/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /Users/hemu/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/hemu/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gpgq3SQK5IOr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def cleanText(text, lemmatize, stemmer):\n",
        "    \"\"\"Method for cleaning text from train and test data. Removes numbers, punctuation, and capitalization. Stems or lemmatizes text.\"\"\"\n",
        "\n",
        "    if isinstance(text, float):\n",
        "        text = str(text)\n",
        "    if isinstance(text, numpy.int64):\n",
        "        text = str(text)\n",
        "    try:\n",
        "        text = text.decode()\n",
        "    except AttributeError:\n",
        "        pass\n",
        "\n",
        "    soup = BeautifulSoup(text, \"lxml\")\n",
        "    text = soup.get_text()\n",
        "    text = re.sub(r\"[^A-Za-z]\", \" \", text)\n",
        "    text = text.lower()\n",
        "\n",
        "\n",
        "    if lemmatize:\n",
        "        wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        def get_tag(tag):\n",
        "            if tag.startswith('J'):\n",
        "                return wordnet.ADJ\n",
        "            elif tag.startswith('V'):\n",
        "                return wordnet.VERB\n",
        "            elif tag.startswith('N'):\n",
        "                return wordnet.NOUN\n",
        "            elif tag.startswith('R'):\n",
        "                return wordnet.ADV\n",
        "            else:\n",
        "                return ''\n",
        "\n",
        "        text_result = []\n",
        "        tokens = word_tokenize(text)  # Generate list of tokens\n",
        "        tagged = pos_tag(tokens)\n",
        "        for t in tagged:\n",
        "            try:\n",
        "                text_result.append(wordnet_lemmatizer.lemmatize(t[0], get_tag(t[1][:2])))\n",
        "            except:\n",
        "                text_result.append(wordnet_lemmatizer.lemmatize(t[0]))\n",
        "        return text_result\n",
        "\n",
        "    if stemmer:\n",
        "        text_result = []\n",
        "        tokens = word_tokenize(text)\n",
        "        snowball_stemmer = SnowballStemmer('english')\n",
        "        for t in tokens:\n",
        "            text_result.append(snowball_stemmer.stem(t))\n",
        "        return text_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuBxfzbuzrs1",
        "outputId": "f68b7f05-9b53-45c3-88af-be598a11d3bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Troubling\n",
            "troubl\n",
            "trouble\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"Troubling\"\n",
        "sample_text_result = cleanText(sample_text, lemmatize=False, stemmer=True)\n",
        "sample_text_result = \" \".join(str(x) for x in sample_text_result)\n",
        "print(sample_text)\n",
        "print(sample_text_result)\n",
        "sample_text_result = cleanText(sample_text, lemmatize=True, stemmer=False)\n",
        "sample_text_result = \" \".join(str(x) for x in sample_text_result)\n",
        "print(sample_text_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcqQi34UoPvq"
      },
      "source": [
        "## Section 1.2: BAG OF WORDS\n",
        "\n",
        "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n",
        "\n",
        "The approach is very simple and flexible, and can be used in many ways for extracting features from documents.\n",
        "\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document.\n",
        "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN8sZqXaNLe7",
        "outputId": "cac9f535-dcb9-488c-a226-dcb69ccecdeb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "5*12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3YYSpQzIM05l"
      },
      "outputs": [],
      "source": [
        "# Functions to convert document(s) to a list of words, with the option of removing stopwords. Returns document-term matrix.\n",
        "\n",
        "def createBagOfWords(train, test, remove_stopwords, lemmatize, stemmer):\n",
        "    if remove_stopwords:\n",
        "        vectorizer = CountVectorizer(analyzer='word', input='content', stop_words=stopwords.words('english'))\n",
        "    else:\n",
        "        vectorizer = CountVectorizer(analyzer='word', input='content')\n",
        "\n",
        "    clean_train = []\n",
        "    for paragraph in train:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_train.append(paragraph)\n",
        "\n",
        "    clean_test = []\n",
        "    for paragraph in test:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_test.append(paragraph)\n",
        "\n",
        "    bag_of_words_train = vectorizer.fit_transform(clean_train).toarray()\n",
        "    bag_of_words_test = vectorizer.transform(clean_test).toarray()\n",
        "    return bag_of_words_train, bag_of_words_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v0iCUBqoX82"
      },
      "source": [
        "## Section 1.3: TF-IDF\n",
        "TF-IDF technique is used to find meaning of sentences consisting of words and cancels out the incapabilities of Bag of Words technique which is good for text classification or for helping a machine read words in numbers.\n",
        "\n",
        "The number of times a term occurs in a document is called its Term frequency (TF).\n",
        "\n",
        " Document frequency is the number of documents in which the word is present.  Inverse DF (IDF) is the inverse of the document frequency which measures the informativeness of term *t*.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "L6_-DDMQoXEX"
      },
      "outputs": [],
      "source": [
        "def createTFIDF(train, test, remove_stopwords, lemmatize, stemmer):\n",
        "    if remove_stopwords:\n",
        "        vectorizer = TfidfVectorizer(analyzer='word', input='content', stop_words=stopwords.words('english'))\n",
        "    else:\n",
        "        vectorizer =  TfidfVectorizer(analyzer='word', input='content')\n",
        "\n",
        "    clean_train = []\n",
        "    for paragraph in train:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_train.append(paragraph)\n",
        "\n",
        "    clean_test = []\n",
        "    for paragraph in test:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_test.append(paragraph)\n",
        "\n",
        "    tfidf_train = vectorizer.fit_transform(clean_train).toarray()\n",
        "    tfidf_test = vectorizer.transform(clean_test).toarray()\n",
        "    return tfidf_train, tfidf_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g0jS45epcC5"
      },
      "source": [
        "# **Section 2: UNDERSTANDING THE DATA : A REVIEWS DATASET**\n",
        "\n",
        "Sentiment analysis is the interpretation and classification of emotions (such as positive, negative and neutral) within text data using text analysis techniques.  \n",
        "Given below is a dataset consisting of reviews along with sentiment class (positive or negative)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "jU6875-NxrHw",
        "outputId": "a3614562-516d-4e8b-9dd2-c5325a061fa8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a2580397-eda7-464f-a19d-451b08cc8e89\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a2580397-eda7-464f-a19d-451b08cc8e89\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving reviews.csv to reviews.csv\n"
          ]
        }
      ],
      "source": [
        "# Upload the Reviews CSV file that has been shared with you.\n",
        "# Run this cell, click on the 'Choose files' button and upload the file.\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HILJMpa_y26e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./reviews.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WXD0iAR5k62v"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "MTCScT7uOdUy",
        "outputId": "df98542e-46c8-45a1-c4c5-f8cd31f16c3b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Not sure who was more lost - the flat characte...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Very little music or anything to speak of.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The best scene in the movie was when Gerardo i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The rest of the movie lacks art, charm, meanin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>I just got bored watching Jessice Lange take h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Unfortunately, any virtue in this film's produ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>In a word, it is embarrassing.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Exceptionally bad!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>All in all its an insult to one's intelligence...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>955 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              sentence sentiment\n",
              "0    Not sure who was more lost - the flat characte...         0\n",
              "1    Attempting artiness with black & white and cle...         0\n",
              "2           Very little music or anything to speak of.         0\n",
              "3    The best scene in the movie was when Gerardo i...         1\n",
              "4    The rest of the movie lacks art, charm, meanin...         0\n",
              "..                                                 ...       ...\n",
              "994  I just got bored watching Jessice Lange take h...         0\n",
              "995  Unfortunately, any virtue in this film's produ...         0\n",
              "996                     In a word, it is embarrassing.         0\n",
              "997                                 Exceptionally bad!         0\n",
              "998  All in all its an insult to one's intelligence...         0\n",
              "\n",
              "[955 rows x 2 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7j9rNGkQpn7y"
      },
      "outputs": [],
      "source": [
        "df.to_csv('./reviews.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QXo11Bxvytu"
      },
      "source": [
        "# **Section 3: KNN MODEL**\n",
        "\n",
        "Given below are two KNN models; in the first case we are using Bag-of-Words and in the second case we are using TF-IDF.\n",
        "Note the different metrics and parameters used in each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import metrics, neighbors\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "\n",
        "## TASK - 1: Tweak the models below and see results with different parameters and distance metrics.\n",
        "\n",
        "def bow_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using bag-of-words and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('./reviews.csv')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"sentence\"], training_data[\"sentiment\"], test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createBagOfWords(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "    # print(X_train)\n",
        "    \n",
        "    metric = [\"minkowski\", \"manhattan\", \"chebyshev\", \"hamming\", \"cosine\", \"euclidean\"]\n",
        "    for _ in metric:\n",
        "        knn11 = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric=_, metric_params=None, n_jobs=1)\n",
        "        knn11.fit(X_train, y_train)\n",
        "        predicted11 = knn11.predict(X_test)\n",
        "        acc11 = metrics.accuracy_score(y_test, predicted11)\n",
        "        print('KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics = ',_,') = ' + str(acc11 * 100) + '%')\n",
        "        scores11 = cross_val_score(knn11, X_train, y_train, cv=3)\n",
        "        print(\"Cross Validation Accuracy: %0.2f\" % (scores11.mean()))\n",
        "        print(scores11)\n",
        "        print('\\n')\n",
        "        \n",
        "        knn12 = neighbors.KNeighborsClassifier(n_neighbors=7, weights='distance', algorithm='auto', leaf_size=30, p=2, metric=_, metric_params=None, n_jobs=1)\n",
        "        knn12.fit(X_train, y_train)\n",
        "        predicted12 = knn12.predict(X_test)\n",
        "        acc12 = metrics.accuracy_score(y_test, predicted12)\n",
        "        print('KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics = ',_,') = ' + str(acc12 * 100) + '%')\n",
        "        scores12 = cross_val_score(knn12, X_train, y_train, cv=3)\n",
        "        print(\"Cross Validation Accuracy: %0.2f\" % (scores12.mean()))\n",
        "        print(scores12)\n",
        "        print('\\n')\n",
        "    return predicted11, y_test\n",
        "\n",
        "#     knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='euclidean', metric_params=None, n_jobs=1)\n",
        "#     knn.fit(X_train, y_train)\n",
        "#     predicted = knn.predict(X_test)\n",
        "#     acc = metrics.accuracy_score(y_test, predicted)\n",
        "#     print('KNN with BOW accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "#     scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "#     print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "#     print(scores)\n",
        "#     print('\\n')\n",
        "#     return predicted, y_test\n",
        "\n",
        "\n",
        "def tfidf_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using tf-idf and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('reviews.csv')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"sentence\"], training_data[\"sentiment\"],\n",
        "                                                        test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createTFIDF(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "    # print(X_train)\n",
        "\n",
        "    metric = [\"minkowski\", \"hamming\", \"euclidean\",\"cosine\"]\n",
        "    for _ in metric:\n",
        "        knn11 = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric=_, metric_params=None, n_jobs=1)\n",
        "        knn11.fit(X_train, y_train)\n",
        "        predicted11 = knn11.predict(X_test)\n",
        "        acc11 = metrics.accuracy_score(y_test, predicted11)\n",
        "        print('KNN with TFIDF accuracy (N_neighbours = 5, distance = uniform, metrics = ',_,') = ' + str(acc11 * 100) + '%')\n",
        "        scores11 = cross_val_score(knn11, X_train, y_train, cv=3)\n",
        "        print(\"Cross Validation Accuracy: %0.2f\" % (scores11.mean()))\n",
        "        print(scores11)\n",
        "        print('\\n')\n",
        "        \n",
        "        knn12 = neighbors.KNeighborsClassifier(n_neighbors=7, weights='distance', algorithm='auto', leaf_size=30, p=2, metric=_, metric_params=None, n_jobs=1)\n",
        "        knn12.fit(X_train, y_train)\n",
        "        predicted12 = knn12.predict(X_test)\n",
        "        acc12 = metrics.accuracy_score(y_test, predicted12)\n",
        "        print('KNN with TFIDF accuracy (N_neighbours = 7, distance = distance, metrics = ',_,') = ' + str(acc12 * 100) + '%')\n",
        "        scores12 = cross_val_score(knn12, X_train, y_train, cv=3)\n",
        "        print(\"Cross Validation Accuracy: %0.2f\" % (scores12.mean()))\n",
        "        print(scores12)\n",
        "        print('\\n')\n",
        "    return predicted11, y_test\n",
        "\n",
        "#     knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='brute', leaf_size=30, p=2,\n",
        "#                                          metric='cosine', metric_params=None, n_jobs=1)\n",
        "\n",
        "#     knn.fit(X_train, y_train)\n",
        "#     predicted = knn.predict(X_test)\n",
        "#     acc = metrics.accuracy_score(y_test, predicted)\n",
        "#     print('KNN with TFIDF accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "#     scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "#     print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "#     print(scores)\n",
        "#     return predicted, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPuI3wrL-8ZJ"
      },
      "source": [
        "Note: Cross-validation will be discussed in detail in the upcoming lab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzno1rRNHoFT",
        "outputId": "7e68eaca-b563-4f9f-8dbc-3692d3336112"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/1t/x8g_6_2j4r35ynvl5w4n0qqh0000gn/T/ipykernel_15491/34442907.py:24: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"lxml\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics =  minkowski ) = 62.30366492146597%\n",
            "Cross Validation Accuracy: 0.62\n",
            "[0.60784314 0.58431373 0.66141732]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics =  minkowski ) = 65.96858638743456%\n",
            "Cross Validation Accuracy: 0.62\n",
            "[0.60784314 0.58823529 0.67716535]\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics =  manhattan ) = 62.30366492146597%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross Validation Accuracy: 0.62\n",
            "[0.63137255 0.59607843 0.61811024]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics =  manhattan ) = 67.5392670157068%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross Validation Accuracy: 0.62\n",
            "[0.65882353 0.6        0.61417323]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics =  chebyshev ) = 49.73821989528796%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross Validation Accuracy: 0.52\n",
            "[0.5372549  0.50588235 0.51574803]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics =  chebyshev ) = 50.26178010471204%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross Validation Accuracy: 0.52\n",
            "[0.54509804 0.52156863 0.50787402]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics =  hamming ) = 68.06282722513089%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross Validation Accuracy: 0.60\n",
            "[0.6        0.60392157 0.61023622]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics =  hamming ) = 65.96858638743456%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross Validation Accuracy: 0.61\n",
            "[0.63137255 0.6        0.60629921]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics =  cosine ) = 69.63350785340315%\n",
            "Cross Validation Accuracy: 0.71\n",
            "[0.70588235 0.69803922 0.73622047]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics =  cosine ) = 70.15706806282722%\n",
            "Cross Validation Accuracy: 0.71\n",
            "[0.68627451 0.69411765 0.73622047]\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics =  euclidean ) = 62.30366492146597%\n",
            "Cross Validation Accuracy: 0.62\n",
            "[0.60784314 0.58431373 0.66141732]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics =  euclidean ) = 65.96858638743456%\n",
            "Cross Validation Accuracy: 0.62\n",
            "[0.60784314 0.58823529 0.67716535]\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "## KNN accuracy after using BoW\n",
        "predicted, y_test = bow_knn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI5NMP8L-4eW",
        "outputId": "94feb12a-c2c8-4611-b850-b61bd5010cf3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/1t/x8g_6_2j4r35ynvl5w4n0qqh0000gn/T/ipykernel_15491/34442907.py:24: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"lxml\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with TFIDF accuracy (N_neighbours = 5, distance = uniform, metrics =  minkowski ) = 68.58638743455498%\n",
            "Cross Validation Accuracy: 0.73\n",
            "[0.73333333 0.74117647 0.70472441]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 7, distance = distance, metrics =  minkowski ) = 71.72774869109948%\n",
            "Cross Validation Accuracy: 0.74\n",
            "[0.70980392 0.75686275 0.7519685 ]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 5, distance = uniform, metrics =  hamming ) = 58.1151832460733%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross Validation Accuracy: 0.54\n",
            "[0.52941176 0.51764706 0.57086614]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 7, distance = distance, metrics =  hamming ) = 58.1151832460733%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross Validation Accuracy: 0.54\n",
            "[0.5372549  0.53333333 0.56299213]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 5, distance = uniform, metrics =  euclidean ) = 68.58638743455498%\n",
            "Cross Validation Accuracy: 0.73\n",
            "[0.73333333 0.74117647 0.70472441]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 7, distance = distance, metrics =  euclidean ) = 71.72774869109948%\n",
            "Cross Validation Accuracy: 0.74\n",
            "[0.70980392 0.75686275 0.7519685 ]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 5, distance = uniform, metrics =  cosine ) = 70.68062827225131%\n",
            "Cross Validation Accuracy: 0.73\n",
            "[0.7254902  0.7372549  0.72440945]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 7, distance = distance, metrics =  cosine ) = 70.15706806282722%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n",
            "/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.10/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross Validation Accuracy: 0.75\n",
            "[0.70588235 0.74901961 0.79133858]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## KNN accuracy after using TFIDF\n",
        "predicted, y_test = tfidf_knn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm2vYDVfa5AP"
      },
      "source": [
        "# Section 4: SPAM TEXT DATASET\n",
        "Now let's use what we've learnt to classify texts as spam or not spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "c1GZgvXCsKX1",
        "outputId": "9774a61d-c0c0-486e-d03b-8dd7112c885f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-73e78393-951b-41f0-b43e-ecf9fc385d43\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-73e78393-951b-41f0-b43e-ecf9fc385d43\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving spam.csv to spam.csv\n"
          ]
        }
      ],
      "source": [
        "# Upload the spam text data CSV file that has been shared with you. You can also download the file from https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset\n",
        "# Run this cell, click on the 'Choose files' button and upload the file.\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "qRiS7dT7piTE",
        "outputId": "95c9117b-0a0d-4745-d57c-05e05ea90293"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will Ã¼ b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Category                                            Message\n",
              "0         ham  Go until jurong point, crazy.. Available only ...\n",
              "1         ham                      Ok lar... Joking wif u oni...\n",
              "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3         ham  U dun say so early hor... U c already then say...\n",
              "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...       ...                                                ...\n",
              "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
              "5568      ham              Will Ã¼ b going to esplanade fr home?\n",
              "5569      ham  Pity, * was in mood for that. So...any other s...\n",
              "5570      ham  The guy did some bitching but I acted like i'd...\n",
              "5571      ham                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./spam.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "OsvBm4luNCME"
      },
      "outputs": [],
      "source": [
        "df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KiEHuMypWyb6",
        "outputId": "06597c9b-19e5-428e-8aaf-60f36b20979d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Category                                            Message\n",
              "0         0  Go until jurong point, crazy.. Available only ...\n",
              "1         0                      Ok lar... Joking wif u oni...\n",
              "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3         0  U dun say so early hor... U c already then say...\n",
              "4         0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRJU9rFy1XQR",
        "outputId": "30c35749-879f-43f9-d62e-bc653a7cdc10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5572"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Wnv0v4T6sqJQ"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics, neighbors\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "\n",
        "## TASK - 2: Tweak the models below and see results with different parameters and distance metrics.\n",
        "\n",
        "def bow_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using bag-of-words and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('spam.csv')\n",
        "    training_data['Category'] = training_data['Category'].map({'ham': 0, 'spam': 1})\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"Message\"], training_data[\"Category\"], test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createBagOfWords(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "\n",
        "    metric = [\"minkowski\", \"manhattan\", \"chebyshev\", \"hamming\", \"cosine\", \"euclidean\"]\n",
        "    for _ in metric:\n",
        "        knn11 = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric=_, metric_params=None, n_jobs=1)\n",
        "        knn11.fit(X_train, y_train)\n",
        "        predicted11 = knn11.predict(X_test)\n",
        "        acc11 = metrics.accuracy_score(y_test, predicted11)\n",
        "        print('KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics = ',_,') = ' + str(acc11 * 100) + '%')\n",
        "        scores11 = cross_val_score(knn11, X_train, y_train, cv=3)\n",
        "        print(\"Cross Validation Accuracy: %0.2f\" % (scores11.mean()))\n",
        "        print(scores11)\n",
        "        print('\\n')\n",
        "        \n",
        "        knn12 = neighbors.KNeighborsClassifier(n_neighbors=7, weights='distance', algorithm='auto', leaf_size=30, p=2, metric=_, metric_params=None, n_jobs=1)\n",
        "        knn12.fit(X_train, y_train)\n",
        "        predicted12 = knn12.predict(X_test)\n",
        "        acc12 = metrics.accuracy_score(y_test, predicted12)\n",
        "        print('KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics = ',_,') = ' + str(acc12 * 100) + '%')\n",
        "        scores12 = cross_val_score(knn12, X_train, y_train, cv=3)\n",
        "        print(\"Cross Validation Accuracy: %0.2f\" % (scores12.mean()))\n",
        "        print(scores12)\n",
        "        print('\\n')\n",
        "    return predicted11, y_test\n",
        "\n",
        "    # knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='euclidean', metric_params=None, n_jobs=1)\n",
        "\n",
        "    # knn.fit(X_train, y_train)\n",
        "    # predicted = knn.predict(X_test)\n",
        "    # acc = metrics.accuracy_score(y_test, predicted)\n",
        "    # print('KNN with BOW accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    # scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    # print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    # print(scores)\n",
        "    # print('\\n')\n",
        "    # return predicted, y_test\n",
        "\n",
        "\n",
        "def tfidf_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using tf-idf and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('spam.csv')\n",
        "    training_data['Category'] = training_data['Category'].map({'ham': 0, 'spam': 1})\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"Message\"], training_data[\"Category\"], test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createTFIDF(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "\n",
        "    metric = [\"minkowski\", \"hamming\", \"euclidean\",\"cosine\"]\n",
        "    for _ in metric:\n",
        "        knn11 = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric=_, metric_params=None, n_jobs=1)\n",
        "        knn11.fit(X_train, y_train)\n",
        "        predicted11 = knn11.predict(X_test)\n",
        "        acc11 = metrics.accuracy_score(y_test, predicted11)\n",
        "        print('KNN with TFIDF accuracy (N_neighbours = 5, distance = uniform, metrics = ',_,') = ' + str(acc11 * 100) + '%')\n",
        "        scores11 = cross_val_score(knn11, X_train, y_train, cv=3)\n",
        "        print(\"Cross Validation Accuracy: %0.2f\" % (scores11.mean()))\n",
        "        print(scores11)\n",
        "        print('\\n')\n",
        "        \n",
        "        knn12 = neighbors.KNeighborsClassifier(n_neighbors=7, weights='distance', algorithm='auto', leaf_size=30, p=2, metric=_, metric_params=None, n_jobs=1)\n",
        "        knn12.fit(X_train, y_train)\n",
        "        predicted12 = knn12.predict(X_test)\n",
        "        acc12 = metrics.accuracy_score(y_test, predicted12)\n",
        "        print('KNN with TFIDF accuracy (N_neighbours = 7, distance = distance, metrics = ',_,') = ' + str(acc12 * 100) + '%')\n",
        "        scores12 = cross_val_score(knn12, X_train, y_train, cv=3)\n",
        "        print(\"Cross Validation Accuracy: %0.2f\" % (scores12.mean()))\n",
        "        print(scores12)\n",
        "        print('\\n')\n",
        "    return predicted11, y_test\n",
        "\n",
        "    # knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='brute', leaf_size=30, p=2, metric='cosine', metric_params=None, n_jobs=1)\n",
        "\n",
        "    # knn.fit(X_train, y_train)\n",
        "    # predicted = knn.predict(X_test)\n",
        "    # acc = metrics.accuracy_score(y_test, predicted)\n",
        "    # print('KNN with TFIDF accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    # scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    # print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    # print(scores)\n",
        "    # return predicted, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8PwydHYs1h_",
        "outputId": "e3e7f314-f47d-421a-9c6d-25085a3cd264"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/1t/x8g_6_2j4r35ynvl5w4n0qqh0000gn/T/ipykernel_15491/34442907.py:24: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"lxml\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics =  minkowski ) = 92.19730941704036%\n",
            "Cross Validation Accuracy: 0.91\n",
            "[0.90713324 0.90040377 0.91245791]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics =  minkowski ) = 93.09417040358744%\n",
            "Cross Validation Accuracy: 0.92\n",
            "[0.92664872 0.9179004  0.92659933]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics =  manhattan ) = 91.74887892376682%\n",
            "Cross Validation Accuracy: 0.90\n",
            "[0.90309556 0.89771198 0.9030303 ]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics =  manhattan ) = 93.45291479820628%\n",
            "Cross Validation Accuracy: 0.93\n",
            "[0.93001346 0.92059219 0.92929293]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics =  chebyshev ) = 90.94170403587444%\n",
            "Cross Validation Accuracy: 0.90\n",
            "[0.90982503 0.88963661 0.9023569 ]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics =  chebyshev ) = 93.27354260089686%\n",
            "Cross Validation Accuracy: 0.92\n",
            "[0.93001346 0.91520861 0.92188552]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics =  hamming ) = 91.56950672645739%\n",
            "Cross Validation Accuracy: 0.90\n",
            "[0.90107672 0.89771198 0.9003367 ]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics =  hamming ) = 93.18385650224215%\n",
            "Cross Validation Accuracy: 0.93\n",
            "[0.93001346 0.92059219 0.92861953]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics =  cosine ) = 97.9372197309417%\n",
            "Cross Validation Accuracy: 0.96\n",
            "[0.95827725 0.95962315 0.96498316]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics =  cosine ) = 97.9372197309417%\n",
            "Cross Validation Accuracy: 0.97\n",
            "[0.96635262 0.9730821  0.96969697]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 5, distance = uniform, metrics =  euclidean ) = 92.19730941704036%\n",
            "Cross Validation Accuracy: 0.91\n",
            "[0.90713324 0.90040377 0.91245791]\n",
            "\n",
            "\n",
            "KNN with BOW accuracy (N_neighbours = 7, distance = distance, metrics =  euclidean ) = 93.09417040358744%\n",
            "Cross Validation Accuracy: 0.92\n",
            "[0.92664872 0.9179004  0.92659933]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# This cell may take some time to run\n",
        "predicted, y_test = bow_knn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf8i2P1nxpl8",
        "outputId": "2290590f-6128-4eae-8037-b2fc299b8c03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/1t/x8g_6_2j4r35ynvl5w4n0qqh0000gn/T/ipykernel_15491/34442907.py:24: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"lxml\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN with TFIDF accuracy (N_neighbours = 5, distance = uniform, metrics =  minkowski ) = 92.19730941704036%\n",
            "Cross Validation Accuracy: 0.90\n",
            "[0.90040377 0.89771198 0.9010101 ]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 7, distance = distance, metrics =  minkowski ) = 93.36322869955157%\n",
            "Cross Validation Accuracy: 0.92\n",
            "[0.92261104 0.91386272 0.91919192]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 5, distance = uniform, metrics =  hamming ) = 87.71300448430493%\n",
            "Cross Validation Accuracy: 0.87\n",
            "[0.87146703 0.86810229 0.87003367]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 7, distance = distance, metrics =  hamming ) = 90.22421524663677%\n",
            "Cross Validation Accuracy: 0.90\n",
            "[0.90780619 0.90040377 0.9003367 ]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 5, distance = uniform, metrics =  euclidean ) = 92.19730941704036%\n",
            "Cross Validation Accuracy: 0.90\n",
            "[0.90040377 0.89771198 0.9010101 ]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 7, distance = distance, metrics =  euclidean ) = 93.36322869955157%\n",
            "Cross Validation Accuracy: 0.92\n",
            "[0.92261104 0.91386272 0.91919192]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 5, distance = uniform, metrics =  cosine ) = 98.11659192825111%\n",
            "Cross Validation Accuracy: 0.96\n",
            "[0.95962315 0.96164199 0.96094276]\n",
            "\n",
            "\n",
            "KNN with TFIDF accuracy (N_neighbours = 7, distance = distance, metrics =  cosine ) = 98.29596412556054%\n",
            "Cross Validation Accuracy: 0.97\n",
            "[0.96298789 0.9717362  0.96498316]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# This cell may take some time to run\n",
        "predicted, y_test = tfidf_knn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9xvbzS4yLTr"
      },
      "source": [
        "### Questions to Think About and Answer\n",
        "1. Why does the TF-IDF approach generally result in a better accuracy than Bag-of-Words ?\n",
        "2. Can you think of techniques that are better than both BoW and TF-IDF ?\n",
        "3. Read about Stemming and Lemmatization from the resources given below. Think about the pros/cons of each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. A. The TF-IDF (Term Frequency-Inverse Document Frequency) approach generally results in better accuracy than the Bag-of-Words (BoW) model due to the following reasons:<br>\n",
        "<br>\n",
        "a. Term Weighting: In the BoW model, all words are treated equally regardless of their importance in the document. However, TF-IDF assigns a weight to each word which signifies the importance of the word in the document. Words that are frequently appearing in a document but not across documents will have a high TF-IDF score, thus helping with the problem of word importance.<br>\n",
        "<br>\n",
        "b. Handling of Rare Words: Rare words will have a higher score in the TF-IDF model, which can help in cases like spam detection where certain rare words might be of high importance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. A. Yes, there are several techniques that are often used in Natural Language Processing tasks that can outperform both Bag-of-Words (BoW) and TF-IDF:<br>\n",
        "<br>\n",
        "a. Word Embeddings: Word embeddings such as Word2Vec, GloVe, and FastText provide a dense vector representation of words where the semantic similarity between words is captured in the vector space. These models are trained to understand the context in which words appear, so words with similar meanings will have vectors that are close together in the vector space.<br>\n",
        "<br>\n",
        "b. Doc2Vec: An extension of Word2Vec, Doc2Vec considers the document level context to generate vectors. It’s useful when we don’t just care about single words, but their context or semantics at the document level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. A. Stemming:<br>\n",
        "<br>\n",
        "* Pros:<br>\n",
        "a. It’s a simpler and faster method because it operates on a single word without understanding the context, and that makes it computationally less intensive.<br>\n",
        "b. It’s useful when the exact word form doesn’t matter as much as the root of the word.<br>\n",
        "<br>\n",
        "* Cons:<br>\n",
        "a. Stemming can often create non-existent words, as it applies a set of language-specific rules to chop off the ends of words. <br>\n",
        "b. It may not be as precise as lemmatization because it simply removes the last few characters of a word, which can lead to loss of meaning.<br>\n",
        "<br>\n",
        "Lemmatization:<br>\n",
        "<br>\n",
        "* Pros:<br>\n",
        "a. It reduces words to their base or root form, known as a lemma, which is linguistically correct.  <br>\n",
        "b. It understands the context of words in text, which leads to more accurate results. <br>\n",
        "<br>\n",
        "* Cons:<br>\n",
        "a. It’s computationally more intensive than stemming, as it involves looking up the lemma in a dictionary and part-of-speech tagging.<br>\n",
        "b. It takes more time than stemming because it needs to find a meaningful representation of the word.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6xPL6smyWG-"
      },
      "source": [
        "### Useful Resources for further reading\n",
        "1. Stemming and Lemmatization: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
        "2. TF-IDF and BoW : https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
        "3. TF-IDF: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
